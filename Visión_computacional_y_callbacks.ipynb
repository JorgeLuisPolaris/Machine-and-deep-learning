{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Visión computacional y callbacks",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNrGrjqEmGAPbwnMVDuxKi7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JorgeLuisPolaris/Machine-and-deep-learning/blob/main/Visi%C3%B3n_computacional_y_callbacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXf7V1s7T8bc"
      },
      "source": [
        "# Ejemplo de visión computacional.\r\n",
        "---\r\n",
        "Empezamos realizando las importaciones necesarias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buDh1GVJURUn"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDJF-rriU8ov"
      },
      "source": [
        "# Carga de datos\r\n",
        "---\r\n",
        "\r\n",
        "Los datos del dataset de Fashion MNIST están disponibles directamente en la API de conjuntos de datos de tf.keras y se cargan de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjBI36GTVPzc"
      },
      "source": [
        "mnist = tf.keras.datasets.fashion_mnist"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ1-SoTDVY0V"
      },
      "source": [
        "Llamar a load_data en este objeto regresará dos conjuntos de dos listas cada uno, estos serán los valores de entrenamiento y prueba para los gráficos que contienen las prendas y sus etiquetas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkPz4mLZVlKZ",
        "outputId": "2eb61f4e-5c5a-40d5-90e1-3605bb8b157f"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-D4AEfdV0k5"
      },
      "source": [
        "# Manipulación de los datos.\r\n",
        "---\r\n",
        "Para poder ver los valores imprimamos una imagen de entrenamiento y una etiqueta de entrenamiento para observarlos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "wVsbDt_TWAHu",
        "outputId": "4d4c01a3-0043-4578-b6bf-8ccf747993e5"
      },
      "source": [
        "import numpy as np\r\n",
        "np.set_printoptions(linewidth=200)\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "plt.imshow(training_images[1])\r\n",
        "print(training_labels[1])\r\n",
        "print(training_images[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "[[  0   0   0   0   0   1   0   0   0   0  41 188 103  54  48  43  87 168 133  16   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   1   0   0   0  49 136 219 216 228 236 255 255 255 255 217 215 254 231 160  45   0   0   0   0   0]\n",
            " [  0   0   0   0   0  14 176 222 224 212 203 198 196 200 215 204 202 201 201 201 209 218 224 164   0   0   0   0]\n",
            " [  0   0   0   0   0 188 219 200 198 202 198 199 199 201 196 198 198 200 200 200 200 201 200 225  41   0   0   0]\n",
            " [  0   0   0   0  51 219 199 203 203 212 238 248 250 245 249 246 247 252 248 235 207 203 203 222 140   0   0   0]\n",
            " [  0   0   0   0 116 226 206 204 207 204 101  75  47  73  48  50  45  51  63 113 222 202 206 220 224   0   0   0]\n",
            " [  0   0   0   0 200 222 209 203 215 200   0  70  98   0 103  59  68  71  49   0 219 206 214 210 250  38   0   0]\n",
            " [  0   0   0   0 247 218 212 210 215 214   0 254 243 139 255 174 251 255 205   0 215 217 214 208 220  95   0   0]\n",
            " [  0   0   0  45 226 214 214 215 224 205   0  42  35  60  16  17  12  13  70   0 189 216 212 206 212 156   0   0]\n",
            " [  0   0   0 164 235 214 211 220 216 201  52  71  89  94  83  78  70  76  92  87 206 207 222 213 219 208   0   0]\n",
            " [  0   0   0 106 187 223 237 248 211 198 252 250 248 245 248 252 253 250 252 239 201 212 225 215 193 113   0   0]\n",
            " [  0   0   0   0   0  17  54 159 222 193 208 192 197 200 200 200 200 201 203 195 210 165   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  47 225 192 214 203 206 204 204 205 206 204 212 197 218 107   0   0   0   0   0   0]\n",
            " [  0   0   0   0   1   6   0  46 212 195 212 202 206 205 204 205 206 204 212 200 218  91   0   3   1   0   0   0]\n",
            " [  0   0   0   0   0   1   0  11 197 199 205 202 205 206 204 205 207 204 205 205 218  77   0   5   0   0   0   0]\n",
            " [  0   0   0   0   0   3   0   2 191 198 201 205 206 205 205 206 209 206 199 209 219  74   0   5   0   0   0   0]\n",
            " [  0   0   0   0   0   2   0   0 188 197 200 207 207 204 207 207 210 208 198 207 221  72   0   4   0   0   0   0]\n",
            " [  0   0   0   0   0   2   0   0 215 198 203 206 208 205 207 207 210 208 200 202 222  75   0   4   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 212 198 209 206 209 206 208 207 211 206 205 198 221  80   0   3   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 204 201 205 208 207 205 211 205 210 210 209 195 221  96   0   3   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 202 201 205 209 207 205 213 206 210 209 210 194 217 105   0   2   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 204 204 205 208 207 205 215 207 210 208 211 193 213 115   0   2   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 204 207 207 208 206 206 215 210 210 207 212 195 210 118   0   2   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 198 208 208 208 204 207 212 212 210 207 211 196 207 121   0   1   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 198 210 207 208 206 209 213 212 211 207 210 197 207 124   0   1   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 172 210 203 201 199 204 207 205 204 201 205 197 206 127   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 188 221 214 234 236 238 244 244 244 240 243 214 224 162   0   2   0   0   0   0]\n",
            " [  0   0   0   0   0   1   0   0 139 146 130 135 135 137 125 124 125 121 119 114 130  76   0   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfdkk6-tWgpM"
      },
      "source": [
        "Podemos observar los resultados y darnos cuenta de que las etiquetas se encuentran en número en lugar de palabras, esto es debido a que las computadoras trabajan mejor con los números y además ayudan a reducir el bias del lenguaje que usamos para nombrarlas, también se notará que todos los valores de valores en los pixeles están entre 0 y 255. Si estamos entrenando una red neuronal, por varias razones es más fácil si tratamos todos los valores entre 0 y 1, un proceso llamado \"normalización\". Afortunadamente en Python es fácil normalizar una lista como esta sin hacer bucles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxngGys1YCQV"
      },
      "source": [
        "training_images  = training_images / 255.0\r\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyk0L9pyYL6P"
      },
      "source": [
        "# Diseño del modelo de la red neuronal.\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrm5gNOqYhJJ"
      },
      "source": [
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\r\n",
        "                                    tf.keras.layers.Dense(128, activation= tf.nn.relu),\r\n",
        "                                    tf.keras.layers.Dense(10,activation= tf.nn.softmax)])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rvEBfBSZhuJ"
      },
      "source": [
        "***Sequential:*** Una secuencia de capas de una red neuronal.\r\n",
        "\r\n",
        "***Flatten:*** Pasa los datos de su forma de imagen cuadrada a un set unidimensional.\r\n",
        "\r\n",
        "***Dense:*** Añade una cada a la red neuronal, cada capa necesita de una función de activación 'activation' pero se usarán estas por ahora.\r\n",
        "\r\n",
        "***Relu:*** Significa que 'Si X > 0 regresa X, si no regresa 0'\r\n",
        "\r\n",
        "***Softmax*** Toma el valor más grande de un set de valores, por ejemplo si la salida de la última capa es [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], tranforma el resultado a [0,0,0,0,1,0,0,0,0] \r\n",
        "\r\n",
        "Lo siguiente que se debe hacer, ahora que el modelo está definido, es construirlo. Para ello es necesario compilarlo con un optimizador y una función de pérdida para posteriormente entrenarlo llamando a * model.fit * pidiéndole que ajuste sus datos de entrenamiento a sus etiquetas de entrenamiento, es decir, que averigüe la relación entre los datos de entrenamiento y sus etiquetas reales, por lo que en el futuro, si tiene datos que se parecen a los datos de entrenamiento, entonces puede hacer una predicción de cómo se verían esos datos.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GipkAl4Yc3BC",
        "outputId": "d459cd43-494b-4d2f-ee36-b5a11284ef76"
      },
      "source": [
        "model.compile(optimizer = tf.optimizers.Adam(),\r\n",
        "              loss = 'sparse_categorical_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(training_images, training_labels, epochs=100)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2847 - accuracy: 0.8939\n",
            "Epoch 2/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2641 - accuracy: 0.8998\n",
            "Epoch 3/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2528 - accuracy: 0.9063\n",
            "Epoch 4/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2449 - accuracy: 0.9089\n",
            "Epoch 5/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2425 - accuracy: 0.9096\n",
            "Epoch 6/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2280 - accuracy: 0.9141\n",
            "Epoch 7/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2250 - accuracy: 0.9160\n",
            "Epoch 8/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2115 - accuracy: 0.9205\n",
            "Epoch 9/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2081 - accuracy: 0.9223\n",
            "Epoch 10/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2037 - accuracy: 0.9220\n",
            "Epoch 11/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1986 - accuracy: 0.9247\n",
            "Epoch 12/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1913 - accuracy: 0.9292\n",
            "Epoch 13/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1845 - accuracy: 0.9295\n",
            "Epoch 14/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1823 - accuracy: 0.9304\n",
            "Epoch 15/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1758 - accuracy: 0.9324\n",
            "Epoch 16/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1720 - accuracy: 0.9351\n",
            "Epoch 17/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1666 - accuracy: 0.9375\n",
            "Epoch 18/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1627 - accuracy: 0.9389\n",
            "Epoch 19/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1586 - accuracy: 0.9398\n",
            "Epoch 20/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1534 - accuracy: 0.9426\n",
            "Epoch 21/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1516 - accuracy: 0.9437\n",
            "Epoch 22/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1510 - accuracy: 0.9436\n",
            "Epoch 23/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1441 - accuracy: 0.9457\n",
            "Epoch 24/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1394 - accuracy: 0.9481\n",
            "Epoch 25/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1371 - accuracy: 0.9476\n",
            "Epoch 26/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1366 - accuracy: 0.9479\n",
            "Epoch 27/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1291 - accuracy: 0.9523\n",
            "Epoch 28/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1287 - accuracy: 0.9517\n",
            "Epoch 29/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1249 - accuracy: 0.9538\n",
            "Epoch 30/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1267 - accuracy: 0.9524\n",
            "Epoch 31/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1200 - accuracy: 0.9548\n",
            "Epoch 32/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1220 - accuracy: 0.9540\n",
            "Epoch 33/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1143 - accuracy: 0.9581\n",
            "Epoch 34/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1182 - accuracy: 0.9555\n",
            "Epoch 35/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1146 - accuracy: 0.9580\n",
            "Epoch 36/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1100 - accuracy: 0.9582\n",
            "Epoch 37/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1115 - accuracy: 0.9568\n",
            "Epoch 38/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1020 - accuracy: 0.9618\n",
            "Epoch 39/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1071 - accuracy: 0.9602\n",
            "Epoch 40/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0984 - accuracy: 0.9630\n",
            "Epoch 41/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0996 - accuracy: 0.9628\n",
            "Epoch 42/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1005 - accuracy: 0.9627\n",
            "Epoch 43/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0950 - accuracy: 0.9646\n",
            "Epoch 44/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0996 - accuracy: 0.9624\n",
            "Epoch 45/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0961 - accuracy: 0.9644\n",
            "Epoch 46/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0952 - accuracy: 0.9648\n",
            "Epoch 47/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0893 - accuracy: 0.9674\n",
            "Epoch 48/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0935 - accuracy: 0.9648\n",
            "Epoch 49/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0939 - accuracy: 0.9652\n",
            "Epoch 50/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0898 - accuracy: 0.9667\n",
            "Epoch 51/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0821 - accuracy: 0.9689\n",
            "Epoch 52/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0873 - accuracy: 0.9668\n",
            "Epoch 53/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0818 - accuracy: 0.9694\n",
            "Epoch 54/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0835 - accuracy: 0.9694\n",
            "Epoch 55/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0802 - accuracy: 0.9704\n",
            "Epoch 56/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0829 - accuracy: 0.9691\n",
            "Epoch 57/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0777 - accuracy: 0.9704\n",
            "Epoch 58/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0772 - accuracy: 0.9710\n",
            "Epoch 59/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0752 - accuracy: 0.9718\n",
            "Epoch 60/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0741 - accuracy: 0.9726\n",
            "Epoch 61/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0760 - accuracy: 0.9709\n",
            "Epoch 62/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0704 - accuracy: 0.9734\n",
            "Epoch 63/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0719 - accuracy: 0.9724\n",
            "Epoch 64/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0730 - accuracy: 0.9720\n",
            "Epoch 65/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0723 - accuracy: 0.9733\n",
            "Epoch 66/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0689 - accuracy: 0.9743\n",
            "Epoch 67/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0684 - accuracy: 0.9740\n",
            "Epoch 68/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0687 - accuracy: 0.9742\n",
            "Epoch 69/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0668 - accuracy: 0.9742\n",
            "Epoch 70/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0683 - accuracy: 0.9746\n",
            "Epoch 71/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0658 - accuracy: 0.9758\n",
            "Epoch 72/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0667 - accuracy: 0.9753\n",
            "Epoch 73/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0629 - accuracy: 0.9768\n",
            "Epoch 74/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0639 - accuracy: 0.9771\n",
            "Epoch 75/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0626 - accuracy: 0.9763\n",
            "Epoch 76/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0597 - accuracy: 0.9776\n",
            "Epoch 77/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0602 - accuracy: 0.9771\n",
            "Epoch 78/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0592 - accuracy: 0.9788\n",
            "Epoch 79/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0606 - accuracy: 0.9767\n",
            "Epoch 80/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0575 - accuracy: 0.9785\n",
            "Epoch 81/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0551 - accuracy: 0.9795\n",
            "Epoch 82/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0577 - accuracy: 0.9783\n",
            "Epoch 83/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0560 - accuracy: 0.9793\n",
            "Epoch 84/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0542 - accuracy: 0.9798\n",
            "Epoch 85/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0570 - accuracy: 0.9786\n",
            "Epoch 86/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0521 - accuracy: 0.9803\n",
            "Epoch 87/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0512 - accuracy: 0.9816\n",
            "Epoch 88/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0557 - accuracy: 0.9793\n",
            "Epoch 89/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0523 - accuracy: 0.9808\n",
            "Epoch 90/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0510 - accuracy: 0.9812\n",
            "Epoch 91/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0536 - accuracy: 0.9797\n",
            "Epoch 92/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0482 - accuracy: 0.9824\n",
            "Epoch 93/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0530 - accuracy: 0.9808\n",
            "Epoch 94/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0537 - accuracy: 0.9807\n",
            "Epoch 95/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0518 - accuracy: 0.9813\n",
            "Epoch 96/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0522 - accuracy: 0.9807\n",
            "Epoch 97/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0474 - accuracy: 0.9826\n",
            "Epoch 98/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0519 - accuracy: 0.9808\n",
            "Epoch 99/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0485 - accuracy: 0.9817\n",
            "Epoch 100/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0508 - accuracy: 0.9812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff0eb1d2550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kal3f_CPdrTh"
      },
      "source": [
        "#Prueba del modelo\r\n",
        "---\r\n",
        "\r\n",
        "Una vez terminado el entrenamiento podremos ver el nivel de presición de cada época pero cómo podríamos probarlo con información que aún no ha visto nuestro modelo, es por eso que se guardaron imagenes de test para evaluar el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ0iLqD7eOIe",
        "outputId": "babf7e9b-4095-43ad-ad06-0cc06c58edf7"
      },
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 1ms/step - loss: 0.7809 - accuracy: 0.8924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7808898687362671, 0.8924000263214111]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWfKlc-Ap2LG"
      },
      "source": [
        "#Callbacks\r\n",
        "---\r\n",
        "\r\n",
        "Anteriormente al entrenar nuestro modelo pudimos darnos cuenta que la presición llegada a cierto punto en las épocas empezaba a fluctuar y ya no mejoraba significativamente ( o incluso empeoraba ) esto puede darse debido al overfitting, para evitar esto y grandes tiempos de entrenamiento utilizaremos callbacks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlOeATx4uIoG",
        "outputId": "37c060b5-882c-4054-de0f-3ff41a1ee00e"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)\r\n",
        "\r\n",
        "class myCallback(tf.keras.callbacks.Callback):\r\n",
        "  def on_epoch_end(self, epoch, logs={}):\r\n",
        "    if(logs.get('loss')<0.4):\r\n",
        "      print(\"\\nAlcanzado un 60% de presición por lo que se cancela el entrenamiento\")\r\n",
        "      self.model.stop_training = True\r\n",
        "\r\n",
        "callbacks = myCallback()\r\n",
        "mnist = tf.keras.datasets.fashion_mnist\r\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\r\n",
        "training_images=training_images/255.0\r\n",
        "test_images=test_images/255.0\r\n",
        "model = tf.keras.models.Sequential([\r\n",
        "  tf.keras.layers.Flatten(),\r\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n",
        "])\r\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\r\n",
        "model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])\r\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5957\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3715\n",
            "\n",
            "Alcanzado un 60% de presición por lo que se cancela el entrenamiento\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3910\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.39095374941825867"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}